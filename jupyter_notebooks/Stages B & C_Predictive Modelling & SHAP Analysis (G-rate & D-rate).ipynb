{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Python libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import h5py\n",
    "import math\n",
    "import shap\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow import keras\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from keras.models import model_from_json\n",
    "from datetime import timedelta as td, datetime\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define global variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_value = 0.0001 ## LEARNING RATE\n",
    "no_of_runs = 3 ## NO. OF RANDOM INTIALIZATION RUNS\n",
    "train_percent = 0.80 ## PERCENTAGE OF TRAINING DATASET\n",
    "batch_size = [4,8,16] ## BATCH SIZES\n",
    "number_of_epochs = [500] ## EPOCHS\n",
    "COMBINATIONS_LIST = [batch_size,number_of_epochs]\n",
    "COMBINATIONS = list(itertools.product(*COMBINATIONS_LIST))\n",
    "\n",
    "continents_list = ['Asia','Africa','Europe','Global-R','North America','South America']\n",
    "multi_time_steps = [1,3,5,7,9] ## NO. OF MULTI-TIME STEPS (DAYS)\n",
    "MA_day_list = [3,5,7] ## NO. OF MOVING AVERAGE DAYS\n",
    "targets_list = ['G','D'] ## TARGET PARAMETERS\n",
    "crit_MAPE_score = 0.700 ## MAXIMUM ALLOWABLE MEAN ABSOLUTE PERCENTAGE ERROR (MAPE) SCORE (%)\n",
    "tol_runs = 10 ## TOTAL NO. OF TOLERANCE RUNS\n",
    "analysis_folder = '' ## YOUR DESIRED DATA LOCATION FOR RESULTS STORAGE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Import data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFINE_YOUR_OWN_HDF5_LOC = '' ## YOUR DEFINED FILE LOCATION STORING THE PROCESSED HDF5 DATA FILES \n",
    "data_path_location = DEFINE_YOUR_OWN_HDF5_LOC\n",
    "list_h5_files = os.listdir(data_path_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define deep neural network function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_nn(x_data_array, y_data_array, ass_x_array):\n",
    "    \n",
    "    layers_neurons = [int(x_train.shape[1]/2),\n",
    "                      int(x_train.shape[1]/4),\n",
    "                      int(x_train.shape[1]/8),\n",
    "                      1,\n",
    "                      3,\n",
    "                      3,\n",
    "                      y_train.shape[1]]\n",
    "    \n",
    "    first_input = keras.Input(shape=(x_data_array.shape[1], ))\n",
    "    second_dense = keras.layers.Dense(layers_neurons[0],\n",
    "                                     activation='relu')(first_input)\n",
    "    third_dense = keras.layers.Dense(layers_neurons[1],\n",
    "                                     activation='relu')(second_dense)\n",
    "    fourth_dense = keras.layers.Dense(layers_neurons[2],\n",
    "                                      activation='relu')(third_dense)\n",
    "    fifth_dense = keras.layers.Dense(layers_neurons[3],\n",
    "                                      activation='relu')(fourth_dense)\n",
    "    merge_one = keras.layers.concatenate([second_dense,\n",
    "                                          third_dense,\n",
    "                                          fourth_dense,\n",
    "                                          fifth_dense])\n",
    "    \n",
    "    ## Data assimilation component \n",
    "    second_input = keras.Input(shape=(ass_x_array.shape[1], ))\n",
    "    merge_two = keras.layers.concatenate([merge_one,second_input])\n",
    "    \n",
    "    ## merging 1D VD-FCNN with data assimilation component\n",
    "    dummy_output_1 = keras.layers.Dense(layers_neurons[-3], \n",
    "                                        activation='relu')(merge_two)\n",
    "    dummy_output_2 = keras.layers.Dense(layers_neurons[-2], \n",
    "                                        activation='relu')(dummy_output_1)\n",
    "    final_output = keras.layers.Dense(layers_neurons[-1], \n",
    "                                      activation='relu')(dummy_output_2)\n",
    "    \n",
    "    final_model = keras.Model(inputs=[first_input, second_input], \n",
    "                              outputs = final_output)\n",
    "\n",
    "    opt = keras.optimizers.Adam(learning_rate = lr_value)\n",
    "    final_model.compile(optimizer = opt,\n",
    "                        loss = 'mean_squared_error')\n",
    "    final_model.summary()\n",
    "    \n",
    "    return final_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model training and validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DEFINE_YOUR_SUMMARY_FILES_LOC = '' ## YOUR DEFINED FILE LOCATION STORING SUMMARY DATA FILES\n",
    "processed_path_location = DEFINE_YOUR_SUMMARY_FILES_LOC\n",
    "\n",
    "for target_type in targets_list:\n",
    "    final_analysis_path_location = data_path_location + '/' + 'With ' + target_type  + \"assimilation\"\n",
    "    if not os.path.exists(final_analysis_path_location):\n",
    "        os.mkdir(final_analysis_path_location)\n",
    "    for continent in continents_list:\n",
    "        \n",
    "        targets_filename = continent + '_G_D_targets.csv'\n",
    "        final_targets_path = processed_path_location + '/' + targets_filename\n",
    "        data_targets = pd.read_csv(final_targets_path)\n",
    "        DATES_DATA_TARGETS = list(data_targets['Date'])\n",
    "        train_dates_list = DATES_DATA_TARGETS[:int(train_percent*len(DATES_DATA_TARGETS))]\n",
    "        test_dates_list = DATES_DATA_TARGETS[int(train_percent*len(DATES_DATA_TARGETS)):]\n",
    "        \n",
    "        for multi_time_value in multi_time_steps:\n",
    "            \n",
    "            for MA_days in MA_day_list:\n",
    "\n",
    "                h5_file_location = data_path_location + '/' + continent + '_' + str(multi_time_value) + '_day_multi_time_steps-' \\\n",
    "                                   + str(MA_days) + '_days_MA.h5'\n",
    "                data_records_file = h5py.File(h5_file_location, 'r')\n",
    "                x_array = data_records_file.get('X_array')\n",
    "                y_array = data_records_file.get(target_type + '_array')\n",
    "                ass_x_array = data_records_file.get('ASS' + '_' + target_type + '_array')\n",
    "\n",
    "                x_scaler = StandardScaler()\n",
    "                x_array = np.array(x_array).reshape(len(x_array),-1)\n",
    "                scaled_x_array = x_scaler.fit_transform(x_array)\n",
    "\n",
    "                x_train = scaled_x_array[:int(train_percent*len(scaled_x_array))]\n",
    "                y_train = y_array[:int(train_percent*len(y_array))]\n",
    "\n",
    "                x_test = scaled_x_array[int(train_percent*len(scaled_x_array)):]\n",
    "                y_test = y_array[int(train_percent*len(y_array)):]\n",
    "\n",
    "                sep_x_train = ass_x_array[:int(train_percent*len(ass_x_array))]\n",
    "                sep_x_test = ass_x_array[int(train_percent*len(ass_x_array)):]\n",
    "\n",
    "                h5_file_folder = final_analysis_path_location + '/' + continent + '_' + str(multi_time_value) \\\n",
    "                                 + '_day_multi_time_steps-' + str(MA_days) + '_days_MA'\n",
    "                if not os.path.exists(h5_file_folder):\n",
    "                    os.mkdir(h5_file_folder)\n",
    "\n",
    "                directory_models = h5_file_folder + '/' + 'Saved_Models'\n",
    "                if not os.path.exists(directory_models):\n",
    "                    os.makedirs(directory_models)\n",
    "\n",
    "                directory_results = h5_file_folder + '/' + 'Saved_Results'\n",
    "                if not os.path.exists(directory_results):\n",
    "                    os.makedirs(directory_results)\n",
    "\n",
    "                directory_figures = h5_file_folder + '/' + 'Saved_Figures'\n",
    "                if not os.path.exists(directory_figures):\n",
    "                    os.makedirs(directory_figures)\n",
    "                    \n",
    "                directory_text_files = h5_file_folder + '/' + 'Saved_Texts'\n",
    "                if not os.path.exists(directory_text_files):\n",
    "                    os.makedirs(directory_text_files)\n",
    "\n",
    "                train_val_df = pd.DataFrame()\n",
    "                test_df = pd.DataFrame()\n",
    "                training_loss_df = pd.DataFrame()\n",
    "                validation_loss_df = pd.DataFrame()\n",
    "\n",
    "                train_val_df['Dates'] = train_dates_list\n",
    "                train_val_df['Actual'] = [item[0] for item in y_train]\n",
    "                test_df['Dates'] = test_dates_list\n",
    "                test_df['Actual'] = [item[0] for item in y_test]\n",
    "                \n",
    "                ## TO STORE INTERMEDIATE MAPE RESULT SCORES ##\n",
    "                results_text_file_location = directory_text_files + '/' + 'Results_LOG.txt'\n",
    "                \n",
    "                f= open(results_text_file_location,\"w+\")\n",
    "                with open(results_text_file_location, 'w') as f:\n",
    "                    f.write('Commence analysis =D')\n",
    "                    f.write('\\n')\n",
    "                \n",
    "                for item in COMBINATIONS:\n",
    "                    number_of_epochs = item[1]\n",
    "                    bs = item[0]\n",
    "                    \n",
    "                    error_counter = 0\n",
    "                    dummy_mape_tol_value = crit_MAPE_score\n",
    "                    while True:\n",
    "                        deep_model = deep_nn(x_train,\n",
    "                                             y_train,\n",
    "                                             sep_x_train)\n",
    "                        history_model = deep_model.fit([x_train,\n",
    "                                                        sep_x_train],\n",
    "                                                       y_train,\n",
    "                                                       epochs = number_of_epochs,\n",
    "                                                       validation_split = 0.2,\n",
    "                                                       batch_size = bs,\n",
    "                                                       verbose = 2)\n",
    "\n",
    "                        predictions_train = deep_model.predict([x_train,\n",
    "                                                                sep_x_train])\n",
    "                        predictions_test = deep_model.predict([x_test,\n",
    "                                                               sep_x_test])\n",
    "\n",
    "                        MAPE = mean_absolute_percentage_error([item[0] for item in y_test],\n",
    "                                                              [item[0] for item in predictions_test])   \n",
    "                        with open(results_text_file_location, 'a') as f:\n",
    "                            result_lines = 'Batch size = ' + str(bs) + ', Epochs = ' + str(number_of_epochs) + ': MAPE = ' \\\n",
    "                                           + str(round(MAPE,3))\n",
    "                            f.writelines(result_lines)\n",
    "                            f.write('\\n')\n",
    "                        \n",
    "                        ## TO INITIATE SHAP ANALYSIS FOR EXPLAINABLE DEEP LEARNING COMPONENT ##\n",
    "                        if MAPE < dummy_mape_tol_value:\n",
    "                            model_json = deep_model.to_json()\n",
    "                            json_filename = 'Batch size = ' + str(bs) + ', Epochs = ' + str(number_of_epochs) + '.json'\n",
    "                            with open(directory_models + '/' + json_filename, \"w\") as json_file:\n",
    "                                json_file.write(model_json)\n",
    "                            h5_filename = 'Batch size = ' + str(bs) + ', Epochs = ' + str(number_of_epochs) + '.h5'\n",
    "                            deep_model.save_weights(directory_models + '/' + h5_filename)\n",
    "                            explainer = shap.KernelExplainer(deep_model.predict,\n",
    "                                                             x_train)\n",
    "                            shap_values = explainer.shap_values(X_test,\n",
    "                                                                nsamples=100) ## CHANGE SAMPLES ACCORDING DEPENDING ON\n",
    "                                                                              ## TOTAL DATA INSTANCES AVAILABLE IN \n",
    "                                                                              ## TESTING DATASET                            \n",
    "                            break\n",
    "                            \n",
    "                        else:\n",
    "                            error_counter += 1\n",
    "                            \n",
    "                        if error_counter % tol_runs == 0:\n",
    "                            dummy_mape_tol_value += 0.05\n",
    "\n",
    "                        clear_output(wait=True)\n",
    "\n",
    "                    training_loss = history_model.history['loss']\n",
    "                    validation_loss = history_model.history['val_loss']\n",
    "                    training_loss_list = [item for item in training_loss]\n",
    "                    validation_loss_list = [item for item in validation_loss]\n",
    "                    training_loss_df[str(number_of_epochs) + '_epochs_' + str(bs) + '_batch_size'] = training_loss_list\n",
    "                    validation_loss_df[str(number_of_epochs) + '_epochs_' + str(bs) + '_batch_size'] = validation_loss_list\n",
    "\n",
    "                    train_val_df[str(number_of_epochs) + '_epochs_' + str(bs) + '_batch_size'] = [item[0] for item in predictions_train]\n",
    "                    test_df[str(number_of_epochs) + '_epochs_' + str(bs) + '_batch_size'] = [item[0] for item in predictions_test]\n",
    "\n",
    "                training_loss_df.to_csv(directory_results + '/' + 'training_loss.csv',\n",
    "                                        index = False)\n",
    "                validation_loss_df.to_csv(directory_results + '/' + 'validation_loss.csv',\n",
    "                                          index = False)\n",
    "                train_val_df.to_csv(directory_results + '/' + 'train_val_predictions.csv',\n",
    "                                    index = False)\n",
    "                test_df.to_csv(directory_results + '/' + 'test_predictions.csv',\n",
    "                               index = False)\n",
    "                clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
